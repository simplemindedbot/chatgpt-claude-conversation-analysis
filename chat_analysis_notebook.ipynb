{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Analysis Dashboard\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of your processed AI chat conversations.\n",
    "\n",
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:37:52.555232Z",
     "start_time": "2025-08-10T02:37:52.551919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analysis libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting styles\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📊 Analysis libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:38:07.360270Z",
     "start_time": "2025-08-10T02:38:06.891396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed chat data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"2025-07-16 13:18:47+00:00\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f%z\", at position 14598. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 38\u001B[39m\n\u001B[32m     26\u001B[39m combined_df = pd.read_sql_query(\u001B[33m\"\"\"\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[33m                                SELECT r.*,\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[33m                                       mf.content_type,\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     34\u001B[39m \u001B[33m                                         JOIN message_features mf ON r.message_id = mf.message_id\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[33m                                \u001B[39m\u001B[33m\"\"\"\u001B[39m, conn)\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# Convert timestamps\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m combined_df[\u001B[33m'\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_datetime\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtimestamp\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m conversations_df[\u001B[33m'\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m'\u001B[39m] = pd.to_datetime(conversations_df[\u001B[33m'\u001B[39m\u001B[33mtimestamp\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     41\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✅ Loaded \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(conversations_df)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m messages from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(conv_features_df)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m conversations\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/GitHub/chat-analysis/chat_analysis_env/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:1072\u001B[39m, in \u001B[36mto_datetime\u001B[39m\u001B[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001B[39m\n\u001B[32m   1070\u001B[39m         result = arg.map(cache_array)\n\u001B[32m   1071\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1072\u001B[39m         values = \u001B[43mconvert_listlike\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1073\u001B[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001B[32m   1074\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/GitHub/chat-analysis/chat_analysis_env/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:435\u001B[39m, in \u001B[36m_convert_listlike_datetimes\u001B[39m\u001B[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001B[39m\n\u001B[32m    433\u001B[39m \u001B[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001B[39;00m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mformat\u001B[39m != \u001B[33m\"\u001B[39m\u001B[33mmixed\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m435\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_array_strptime_with_fallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexact\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    437\u001B[39m result, tz_parsed = objects_to_datetime64(\n\u001B[32m    438\u001B[39m     arg,\n\u001B[32m    439\u001B[39m     dayfirst=dayfirst,\n\u001B[32m   (...)\u001B[39m\u001B[32m    443\u001B[39m     allow_object=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    444\u001B[39m )\n\u001B[32m    446\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tz_parsed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    447\u001B[39m     \u001B[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001B[39;00m\n\u001B[32m    448\u001B[39m     \u001B[38;5;66;03m# is in UTC\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/GitHub/chat-analysis/chat_analysis_env/lib/python3.13/site-packages/pandas/core/tools/datetimes.py:469\u001B[39m, in \u001B[36m_array_strptime_with_fallback\u001B[39m\u001B[34m(arg, name, utc, fmt, exact, errors)\u001B[39m\n\u001B[32m    458\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_array_strptime_with_fallback\u001B[39m(\n\u001B[32m    459\u001B[39m     arg,\n\u001B[32m    460\u001B[39m     name,\n\u001B[32m   (...)\u001B[39m\u001B[32m    464\u001B[39m     errors: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    465\u001B[39m ) -> Index:\n\u001B[32m    466\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    467\u001B[39m \u001B[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001B[39;00m\n\u001B[32m    468\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m469\u001B[39m     result, tz_out = \u001B[43marray_strptime\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfmt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexact\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexact\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mutc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    470\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m tz_out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    471\u001B[39m         unit = np.datetime_data(result.dtype)[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/tslibs/strptime.pyx:501\u001B[39m, in \u001B[36mpandas._libs.tslibs.strptime.array_strptime\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/tslibs/strptime.pyx:451\u001B[39m, in \u001B[36mpandas._libs.tslibs.strptime.array_strptime\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/tslibs/strptime.pyx:583\u001B[39m, in \u001B[36mpandas._libs.tslibs.strptime._parse_with_format\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mValueError\u001B[39m: time data \"2025-07-16 13:18:47+00:00\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f%z\", at position 14598. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Environment and Kernel check (recommended)\n",
    "import sys\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "print(\"\\n🧪 Environment check:\")\n",
    "print(f\" • Python executable: {sys.executable}\")\n",
    "_in_venv = (getattr(sys, 'base_prefix', sys.prefix) != sys.prefix) or hasattr(sys, 'real_prefix')\n",
    "print(f\" • In virtualenv: {_in_venv}\")\n",
    "# Heuristic: suggest using the repo venv if not active\n",
    "_repo_venv = _Path('chat_analysis_env')\n",
    "if _repo_venv.exists():\n",
    "    expected_name = 'chat_analysis_env'\n",
    "    is_expected = expected_name in sys.executable or expected_name in sys.prefix\n",
    "    if not is_expected:\n",
    "        print(\" ⚠️ Notebook kernel does not appear to be using the 'chat_analysis_env' virtual environment.\")\n",
    "        print(\"    To use the correct environment in Jupyter:\")\n",
    "        print(\"      1) source chat_analysis_env/bin/activate  (Windows: chat_analysis_env\\\\Scripts\\\\activate)\")\n",
    "        print(\"      2) python -m ipykernel install --user --name chat_analysis_env --display-name \\\"Python (chat_analysis_env)\\\"\")\n",
    "        print(\"      3) In Jupyter, select Kernel → Change kernel → Python (chat_analysis_env)\")\n",
    "\n",
    "# Quick dependency/resource checks\n",
    "_missing = []\n",
    "try:\n",
    "    import spacy  # type: ignore\n",
    "    try:\n",
    "        import en_core_web_sm  # type: ignore\n",
    "        _has_spacy_model = True\n",
    "    except Exception:\n",
    "        # Try loading by name in case it's already linked\n",
    "        try:\n",
    "            spacy.load(\"en_core_web_sm\")\n",
    "            _has_spacy_model = True\n",
    "        except Exception:\n",
    "            _has_spacy_model = False\n",
    "    if not _has_spacy_model:\n",
    "        _missing.append(\"spaCy model 'en_core_web_sm'\")\n",
    "except Exception:\n",
    "    _missing.append(\"spaCy package\")\n",
    "\n",
    "try:\n",
    "    import nltk  # type: ignore\n",
    "    try:\n",
    "        nltk.data.find('sentiment/vader_lexicon')\n",
    "        _has_vader = True\n",
    "    except LookupError:\n",
    "        _has_vader = False\n",
    "    if not _has_vader:\n",
    "        _missing.append(\"NLTK VADER lexicon\")\n",
    "except Exception:\n",
    "    _missing.append(\"NLTK package\")\n",
    "\n",
    "# Sentence transformers optional for the notebook, required for embeddings\n",
    "try:\n",
    "    import sentence_transformers  # type: ignore\n",
    "except Exception:\n",
    "    print(\" ℹ️ Optional: 'sentence-transformers' not importable in this kernel. Embedding visualizations may be limited if used here.\")\n",
    "\n",
    "if _missing:\n",
    "    print(\" ⚠️ Missing components detected:\")\n",
    "    for m in _missing:\n",
    "        print(f\"   - {m}\")\n",
    "    print(\" 👉 Install inside the active environment:\")\n",
    "    print(\"    • pip install -r requirements.txt\")\n",
    "    print(\"    • python -m spacy download en_core_web_sm\")\n",
    "    print(\"    • python -c \\\"import nltk; nltk.download('vader_lexicon')\\\"\")\n",
    "\n",
    "# Locate and validate database\n",
    "from pathlib import Path\n",
    "\n",
    "db_path = Path('chat_analysis.db')\n",
    "if not db_path.exists():\n",
    "    print(\"⚠️ chat_analysis.db not found in the project root.\")\n",
    "    print(\"To generate it, run the processing pipeline first:\")\n",
    "    print(\"  1) python normalize_chats.py\")\n",
    "    print(\"  2) python process_runner.py combined_ai_chat_history.csv\")\n",
    "    if Path('combined_ai_chat_history.csv').exists():\n",
    "        print(\"Detected combined_ai_chat_history.csv – you can run step 2 immediately.\")\n",
    "    else:\n",
    "        print(\"No combined_ai_chat_history.csv found. The normalization step will create it from your exports.\")\n",
    "    raise FileNotFoundError(\"chat_analysis.db is required. See instructions above.\")\n",
    "\n",
    "# Connect to the processed database\n",
    "conn = sqlite3.connect(str(db_path))\n",
    "\n",
    "# Verify required tables exist\n",
    "required_tables = {'raw_conversations', 'message_features', 'conversation_features'}\n",
    "existing_tables = set(pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)['name'])\n",
    "missing = required_tables - existing_tables\n",
    "if missing:\n",
    "    print(f\"⚠️ Missing tables in database: {', '.join(sorted(missing))}\")\n",
    "    print(\"Please re-run the pipeline: python process_runner.py combined_ai_chat_history.csv\")\n",
    "    raise RuntimeError(f\"Database is incomplete. Missing tables: {', '.join(sorted(missing))}\")\n",
    "\n",
    "# Load all data tables\n",
    "print(\"Loading processed chat data...\")\n",
    "\n",
    "# Raw conversations\n",
    "conversations_df = pd.read_sql_query(\"\"\"\n",
    "                                     SELECT *\n",
    "                                     FROM raw_conversations\n",
    "                                     \"\"\", conn)\n",
    "\n",
    "# Message features\n",
    "features_df = pd.read_sql_query(\"\"\"\n",
    "                                SELECT *\n",
    "                                FROM message_features\n",
    "                                \"\"\", conn)\n",
    "\n",
    "# Conversation-level features\n",
    "conv_features_df = pd.read_sql_query(\"\"\"\n",
    "                                     SELECT *\n",
    "                                     FROM conversation_features\n",
    "                                     \"\"\", conn)\n",
    "\n",
    "# Combined view\n",
    "combined_df = pd.read_sql_query(\"\"\"\n",
    "                                SELECT r.*,\n",
    "                                       mf.content_type,\n",
    "                                       mf.sentiment_score,\n",
    "                                       mf.has_code,\n",
    "                                       mf.has_urls,\n",
    "                                       mf.has_questions\n",
    "                                FROM raw_conversations r\n",
    "                                         JOIN message_features mf ON r.message_id = mf.message_id\n",
    "                                \"\"\", conn)\n",
    "\n",
    "# Convert timestamps\n",
    "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], utc=True, errors='coerce')\n",
    "conversations_df['timestamp'] = pd.to_datetime(conversations_df['timestamp'], utc=True, errors='coerce')\n",
    "\n",
    "print(f\"✅ Loaded {len(conversations_df):,} messages from {len(conv_features_df):,} conversations\")\n",
    "print(f\"📈 Data spans from {combined_df['timestamp'].min()} to {combined_df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Overview Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== CHAT ANALYSIS OVERVIEW ===\")\n",
    "print(f\"📝 Total Messages: {len(combined_df):,}\")\n",
    "print(f\"💬 Total Conversations: {combined_df['conversation_id'].nunique():,}\")\n",
    "print(f\"🤖 AI Sources: {', '.join(combined_df['source_ai'].unique())}\")\n",
    "print(\n",
    "    f\"📅 Date Range: {combined_df['timestamp'].min().strftime('%Y-%m-%d')} to {combined_df['timestamp'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"📊 Total Words: {combined_df['word_count'].sum():,}\")\n",
    "print(f\"📈 Average Words per Message: {combined_df['word_count'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n=== MESSAGE BREAKDOWN ===\")\n",
    "role_counts = combined_df['role'].value_counts()\n",
    "for role, count in role_counts.items():\n",
    "    print(f\"{role}: {count:,} messages ({count / len(combined_df) * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== CONTENT TYPES ===\")\n",
    "content_counts = combined_df['content_type'].value_counts()\n",
    "for content_type, count in content_counts.items():\n",
    "    print(f\"{content_type.title()}: {count:,} messages ({count / len(combined_df) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Activity Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages over time\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=['Messages Over Time', 'Messages by AI Source', 'Content Types Distribution',\n",
    "                    'Sentiment Distribution'],\n",
    "    specs=[[{\"secondary_y\": True}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"pie\"}, {\"type\": \"histogram\"}]]\n",
    ")\n",
    "\n",
    "# Messages over time by AI source\n",
    "daily_messages = combined_df.groupby([combined_df['timestamp'].dt.date, 'source_ai']).size().reset_index(name='count')\n",
    "for ai_source in combined_df['source_ai'].unique():\n",
    "    ai_data = daily_messages[daily_messages['source_ai'] == ai_source]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ai_data['timestamp'], y=ai_data['count'], name=ai_source, mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Messages by AI source\n",
    "ai_counts = combined_df['source_ai'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=ai_counts.index, y=ai_counts.values, name='AI Sources'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Content types pie chart\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=content_counts.index, values=content_counts.values, name=\"Content Types\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Sentiment distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=combined_df['sentiment_score'], name='Sentiment', nbinsx=30),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Chat Analysis Dashboard\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🕒 Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time-based features\n",
    "combined_df['hour'] = combined_df['timestamp'].dt.hour\n",
    "combined_df['day_of_week'] = combined_df['timestamp'].dt.day_name()\n",
    "combined_df['month'] = combined_df['timestamp'].dt.month_name()\n",
    "\n",
    "# Hourly activity pattern\n",
    "hourly_activity = combined_df.groupby(['hour', 'source_ai']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.line(hourly_activity, x='hour', y='count', color='source_ai',\n",
    "              title='Chat Activity by Hour of Day',\n",
    "              labels={'hour': 'Hour of Day', 'count': 'Number of Messages'})\n",
    "fig.show()\n",
    "\n",
    "# Day of week activity\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_activity = combined_df.groupby(['day_of_week', 'source_ai']).size().reset_index(name='count')\n",
    "daily_activity['day_of_week'] = pd.Categorical(daily_activity['day_of_week'], categories=day_order, ordered=True)\n",
    "daily_activity = daily_activity.sort_values('day_of_week')\n",
    "\n",
    "fig = px.bar(daily_activity, x='day_of_week', y='count', color='source_ai',\n",
    "             title='Chat Activity by Day of Week',\n",
    "             labels={'day_of_week': 'Day of Week', 'count': 'Number of Messages'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💬 Conversation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation lengths and characteristics\n",
    "print(\"=== CONVERSATION CHARACTERISTICS ===\")\n",
    "print(f\"Average messages per conversation: {conv_features_df['message_count'].mean():.1f}\")\n",
    "print(f\"Median messages per conversation: {conv_features_df['message_count'].median():.1f}\")\n",
    "print(f\"Average words per conversation: {conv_features_df['total_word_count'].mean():.0f}\")\n",
    "print(f\"Average conversation duration: {conv_features_df['duration_minutes'].mean():.1f} minutes\")\n",
    "\n",
    "# Conversation types\n",
    "print(\"\\n=== CONVERSATION TYPES ===\")\n",
    "conv_type_counts = conv_features_df['conversation_type'].value_counts()\n",
    "for conv_type, count in conv_type_counts.items():\n",
    "    print(\n",
    "        f\"{conv_type.replace('_', ' ').title()}: {count:,} conversations ({count / len(conv_features_df) * 100:.1f}%)\")\n",
    "\n",
    "# Visualize conversation characteristics\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=['Conversation Length Distribution', 'Duration vs Messages', 'Conversation Types',\n",
    "                    'Complexity vs Idea Density']\n",
    ")\n",
    "\n",
    "# Conversation length distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=conv_features_df['message_count'], name='Message Count', nbinsx=20),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Duration vs Messages scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=conv_features_df['message_count'], y=conv_features_df['duration_minutes'],\n",
    "               mode='markers', name='Duration vs Messages'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Conversation types\n",
    "fig.add_trace(\n",
    "    go.Bar(x=conv_type_counts.index, y=conv_type_counts.values, name='Conversation Types'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Complexity vs. Idea Density\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=conv_features_df['complexity_score'], y=conv_features_df['idea_density'],\n",
    "               mode='markers', name='Complexity vs Density'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Conversation Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content analysis by AI source\n",
    "content_by_ai = combined_df.groupby(['source_ai', 'content_type']).size().reset_index(name='count')\n",
    "content_pivot = content_by_ai.pivot(index='source_ai', columns='content_type', values='count').fillna(0)\n",
    "\n",
    "# Normalize by percentage\n",
    "content_pivot_pct = content_pivot.div(content_pivot.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig = px.bar(content_by_ai, x='source_ai', y='count', color='content_type',\n",
    "             title='Content Types by AI Source',\n",
    "             labels={'source_ai': 'AI Source', 'count': 'Number of Messages'})\n",
    "fig.show()\n",
    "\n",
    "# Code vs non-code analysis\n",
    "code_analysis = combined_df.groupby('source_ai').agg({\n",
    "    'has_code': 'sum',\n",
    "    'has_urls': 'sum',\n",
    "    'has_questions': 'sum',\n",
    "    'message_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "code_analysis['code_percentage'] = (code_analysis['has_code'] / code_analysis['message_id']) * 100\n",
    "code_analysis['url_percentage'] = (code_analysis['has_urls'] / code_analysis['message_id']) * 100\n",
    "code_analysis['question_percentage'] = (code_analysis['has_questions'] / code_analysis['message_id']) * 100\n",
    "\n",
    "print(\"=== CONTENT CHARACTERISTICS BY AI SOURCE ===\")\n",
    "for _, row in code_analysis.iterrows():\n",
    "    print(f\"\\n{row['source_ai']}:\")\n",
    "    print(f\"  Code messages: {row['code_percentage']:.1f}%\")\n",
    "    print(f\"  URL messages: {row['url_percentage']:.1f}%\")\n",
    "    print(f\"  Question messages: {row['question_percentage']:.1f}%\")\n",
    "\n",
    "# Visualize content characteristics\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name='Code', x=code_analysis['source_ai'], y=code_analysis['code_percentage']))\n",
    "fig.add_trace(go.Bar(name='URLs', x=code_analysis['source_ai'], y=code_analysis['url_percentage']))\n",
    "fig.add_trace(go.Bar(name='Questions', x=code_analysis['source_ai'], y=code_analysis['question_percentage']))\n",
    "fig.update_layout(title='Content Characteristics by AI Source (%)', barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 😊 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis by AI and content type\n",
    "sentiment_stats = combined_df.groupby(['source_ai', 'role']).agg({\n",
    "    'sentiment_score': ['mean', 'std', 'count']\n",
    "}).round(3)\n",
    "\n",
    "print(\"=== SENTIMENT ANALYSIS ===\")\n",
    "print(sentiment_stats)\n",
    "\n",
    "# Sentiment over time\n",
    "combined_df['date'] = combined_df['timestamp'].dt.date\n",
    "daily_sentiment = combined_df.groupby(['date', 'source_ai'])['sentiment_score'].mean().reset_index()\n",
    "\n",
    "fig = px.line(daily_sentiment, x='date', y='sentiment_score', color='source_ai',\n",
    "              title='Average Sentiment Over Time',\n",
    "              labels={'date': 'Date', 'sentiment_score': 'Average Sentiment Score'})\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Neutral\")\n",
    "fig.show()\n",
    "\n",
    "# Sentiment distribution by AI source\n",
    "fig = px.box(combined_df, x='source_ai', y='sentiment_score', color='source_ai',\n",
    "             title='Sentiment Score Distribution by AI Source')\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.show()\n",
    "\n",
    "# Sentiment by content type\n",
    "fig = px.box(combined_df, x='content_type', y='sentiment_score',\n",
    "             title='Sentiment by Content Type')\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 Top Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most productive days\n",
    "daily_counts = combined_df.groupby(combined_df['timestamp'].dt.date).size()\n",
    "top_days = daily_counts.nlargest(10)\n",
    "\n",
    "print(\"=== TOP 10 MOST ACTIVE DAYS ===\")\n",
    "for date, count in top_days.items():\n",
    "    print(f\"{date}: {count} messages\")\n",
    "\n",
    "# Longest conversations\n",
    "longest_convs = conv_features_df.nlargest(10, 'message_count')[\n",
    "    ['title', 'source_ai', 'message_count', 'total_word_count', 'conversation_type']]\n",
    "print(\"\\n=== TOP 10 LONGEST CONVERSATIONS ===\")\n",
    "for _, conv in longest_convs.iterrows():\n",
    "    title = conv['title'][:50] + \"...\" if len(str(conv['title'])) > 50 else conv['title']\n",
    "    print(f\"{title} ({conv['source_ai']})\")\n",
    "    print(f\"  {conv['message_count']} messages, {conv['total_word_count']:,} words, type: {conv['conversation_type']}\")\n",
    "\n",
    "# Most complex conversations (high code and question content)\n",
    "complex_convs = conv_features_df.nlargest(10, 'complexity_score')[\n",
    "    ['title', 'source_ai', 'complexity_score', 'conversation_type']]\n",
    "print(\"\\n=== TOP 10 MOST COMPLEX CONVERSATIONS ===\")\n",
    "for _, conv in complex_convs.iterrows():\n",
    "    title = conv['title'][:50] + \"...\" if len(str(conv['title'])) > 50 else conv['title']\n",
    "    print(f\"{title} ({conv['source_ai']})\")\n",
    "    print(f\"  Complexity: {conv['complexity_score']:.2f}, Type: {conv['conversation_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Custom Analysis\n",
    "\n",
    "Use this section to run your own custom queries and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find all conversations about a specific topic\n",
    "search_term = \"python\"  # Change this to search for specific topics\n",
    "\n",
    "topic_conversations = combined_df[combined_df['content'].str.contains(search_term, case=False, na=False)]\n",
    "topic_conv_ids = topic_conversations['conversation_id'].unique()\n",
    "\n",
    "print(f\"Found {len(topic_conv_ids)} conversations mentioning '{search_term}'\")\n",
    "print(f\"Total messages in these conversations: {len(topic_conversations)}\")\n",
    "\n",
    "# Show some examples\n",
    "if len(topic_conv_ids) > 0:\n",
    "    sample_convs = conv_features_df[conv_features_df['conversation_id'].isin(topic_conv_ids)].head(5)\n",
    "    print(f\"\\nSample conversations:\")\n",
    "    for _, conv in sample_convs.iterrows():\n",
    "        title = conv['title'][:60] + \"...\" if len(str(conv['title'])) > 60 else conv['title']\n",
    "        print(f\"• {title} ({conv['source_ai']}) - {conv['message_count']} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary data for external use\n",
    "summary_data = {\n",
    "    'total_messages': len(combined_df),\n",
    "    'total_conversations': len(conv_features_df),\n",
    "    'ai_sources': list(combined_df['source_ai'].unique()),\n",
    "    'date_range': {\n",
    "        'start': combined_df['timestamp'].min().isoformat(),\n",
    "        'end': combined_df['timestamp'].max().isoformat()\n",
    "    },\n",
    "    'content_types': content_counts.to_dict(),\n",
    "    'avg_sentiment': combined_df['sentiment_score'].mean(),\n",
    "    'code_percentage': (combined_df['has_code'].sum() / len(combined_df)) * 100\n",
    "}\n",
    "\n",
    "print(\"=== EXPORTABLE SUMMARY ===\")\n",
    "print(json.dumps(summary_data, indent=2, default=str))\n",
    "\n",
    "# Optionally save to a file\n",
    "# with open('chat_analysis_summary.json', 'w') as f:\n",
    "#     json.dump(summary_data, f, indent=2, default=str)\n",
    "# print(\"Summary saved to chat_analysis_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Database Connection Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"✅ Database connection closed\")\n",
    "print(\"🎉 Analysis complete! Feel free to modify and extend this notebook for your specific needs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
